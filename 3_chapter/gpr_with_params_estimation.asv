% ハイパーパラメータ推定ありのガウス過程回帰．
clc
clear
close all 

% kernel parameters
tau = log(1);
sigma = log(1);
% eta = log(1);
eta = log(0.1);

params = [tau sigma eta];

% 学習データ読み込み
data = readmatrix('gpr.dat');
xtrain = zeros(length(data), 1); ytrain = zeros(length(data), 1);
for i = 1:1:length(data)
    xtrain(i,1) = data(i,1);
    ytrain(i,1) = data(i,2);
end


% 確め計算ゾーン -----------------------------------------------------------
% kernel = gaussian_kernel;
gaussian_kernel(xtrain(3,1), ytrain(3,1), params);
kv(xtrain(i,1), xtrain, params);
loglik(params, xtrain, ytrain);
L_grad(params, xtrain, ytrain)
% -------------------------------------------------------------------------


% 回帰の計算
xx = (-1:0.1:4)';
regression = gpr(xx, xtrain, ytrain, params);
mu = regression(:,1); var = regression(:,2);
two_sigma1 = mu - 2 * sqrt(var); two_sigma2 = mu + 2 * sqrt(var);

% plot
f1 = figure;
figure(f1)
patch([xx', fliplr(xx')], [two_sigma1', fliplr(two_sigma2')], 'c');
hold on;
plot(xtrain, ytrain, 'bx', MarkerSize=20);
hold on;
plot(xx, mu, 'b-', LineWidth=2);
title("Gaussian Process Regression with parameters estimation");
save_name = "gpr_with_params_estimation.png";
saveas(gcf, save_name);

% -------------------------------------------------------------------------
% kgauss(x, y);
function delta = delta(x, y)
    if x == y
        delta = 1;
    else
        delta = 0;
    end
end

% デルタ関数考慮のガウスカーネル生成
function gaussian_kernel = gaussian_kernel(x, y, params)
    tau = params(1,1); sigma = params(1,2); eta = params(1,3);
    % 無名関数
    kgauss = @(x, y) exp(tau) * exp(-(x - y)^2 / exp(sigma))...
        + exp(eta) * delta(x, y);
    gaussian_kernel = kgauss(x, y);
end

% ハイパーパラメータに対する，式(3.92)の勾配？dって何？
function kgrad = kgauss_grad(xi, xj, d, params)
    if d == 1
        kgrad = exp(params(1,d)) * gaussian_kernel(xi, xj, params);
    elseif d == 2
        kgrad = gaussian_kernel(xi, xj, params) * (xi - xj) * (xi - xj) / exp(params(1, d));
    elseif d == 3
        if xi == xj
            kgrad = exp(params(1, d));
        else
            kgrad = 0;
        end
    end
end

% 一つの入力xに対するxtrainの列ベクトル．すなわち列が入力x (１次元の入力ならx軸(横軸))，行がxtrainの学習カーネル行列？？
% xtrainは列ベクトル（縦）
function kv = kv(x, xtrain, params) % kernelを無名関数で作ってないから，kernelとして引数で与えられないことに注意．
    kv = zeros(length(xtrain), 1);
    for i = 1:1:length(xtrain)
        kv(i,1) = gaussian_kernel(x, xtrain(i,1), params) ...
            - exp(params(1,3)) * delta(x, xtrain(i,1)); % kvを作るときになんかしてる．．．
    end    
end

% カーネル行列を作る関数
function K = kernel_matrix(xx, params)
    N = length(xx);
    K = zeros(N, N);
    for i = 1:1:N
        for j = 1:1:N
            K(i,j) = gaussian_kernel(xx(i,1), xx(j,1), params);
        end
    end
end

% ガウス過程回帰
function y = gpr(xx, xtrain, ytrain, params)
    K = kernel_matrix(xtrain, params); % 学習データのカーネル行列
    Kinv = inv(K);
    N = length(xx);
    mu = zeros(N, 1); var = zeros(N, 1);
    for i = 1:1:N
        s = gaussian_kernel(xx(i,1), xx(i,1), params); % カーネル行列 k_** を作ってる．
        k = kv(xx(i,1), xtrain, params); % 縦ベクトル (回帰する点のカーネル行列k^*?)
        mu(i,1) = k' * Kinv * ytrain;
        var(i,1) = s - k' * Kinv * k;
    end
    y = [mu var]; % 回帰した平均と分散？
end

% trace (対角和) ではないかもしれない．なんだこれ → いや対角和っぽい
function trace = tr(A, B) % AとBは行列
    trace = sum(A * B', "all");
end

% hiper parameters を未知のthetaと置いたときの学習データの確率pのlogをとったやつ
function L = loglik(params, xtrain, ytrain)
    K = kernel_matrix(xtrain, params);
    Kinv = inv(K);
    L = log(det(K)) + ytrain' * Kinv * ytrain; % 教科書はマイナスだけどなんかプラスにしてる
end

% Lのハイパーパラメータthetaに対する勾配
% function grad = gradient(params, xtrain, ytrain)　gradient関数がmatlabの中に既にあったからエラー？
function grad = L_grad(params, xtrain, ytrain) % kernel=gaussian_kernel(), kgrad = kgauss_grad()
    K = kernel_matrix(xtrain, params);
    Kinv = inv(K);
    Kinvy = Kinv * ytrain;
    D = length(params);
    N = length(xtrain);
    grad = zeros(D, 1);
    G = zeros(N, N);
    for d = 1:1:D
        for i = 1:1:N
            for j = 1:1:N
                % Gはカーネル行列の各要素をθベクトルの各要素で微分した行列 (p91)
                G(i,j) = kgauss_grad(xtrain(i,1), xtrain(j,1), d, params);
            end
        end
        size(tr(Kinv, G));
        grad(d,1) = tr(Kinv, G) - Kinvy' * G * Kinvy;
    end
end

% numgradが何をしているのかわかってない... → 使ってなさそう？？
function ngrad = numgrad(params, xtrain, ytrain, eps)
    arguments
        % 引数のデフォルト値設定
        params; xtrain; ytrain; eps = 1e-6;
    end

    D = length(params);
    ngrad = zeros(D, 1);
    for d = 1:1:D
        lik = loglik(prams, xtrain, ytrain);
        params(1,d) = params(1,d) + eps;
        newlik = loglik(params, xtrain, ytrain);
        params(1,d) = params(1,d) - eps; % なんだこれまじで
        ngrad(d, 1) = (newlik - lik) / eps;
    end
end

% ここから勾配法の最適化？
function res = optimize(xtrain, ytrain, init) % ini



% function ans = main()
% end



